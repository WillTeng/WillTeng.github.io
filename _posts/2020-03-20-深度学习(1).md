---
layout: post
title:  "Pytorch实现线性回归"
categories: 深度学习
tags:  Pytorch
author: 邓威
---

* content
{:toc}

## 定义网络模型
def line_reg(x, w, b):
    return t.mm(x, w) + b
    pass
    
## 定义损失函数
def loss(true_y, y):
    return (true_y - y.view(true_y.size())) ** 2 / 2
    pass
    
## 定义优化函数
def optimize(params, lr, batch):
    # print(type(params))
    for param in params:
        print(param.data)
        param.data -= lr * param.grad / batch
    pass
    
## 初始化模型
w = t.tensor(np.random.normal(0, 0.1, (2, 1)), dtype=t.float32, requires_grad=True)
b = t.zeros(1, dtype=t.float32, requires_grad=True)

## 训练和测试
epochs = 4
    lr = 0.03
    batch = 10
    for epoch in range(epochs):
        for x, y in get_random_sample(batch, features, labels):
            l = loss(line_reg(x, w, b), y).sum()
            l.backward()
            # print('before', w.grad.data)
            optimize([w, b], lr, batch)
            # print('after', w.grad.data)
            w.grad.data.zero_()
            b.grad.data.zero_()
        train_1 = loss(line_reg(features, w, b), labels)
        print('epoch %d,loss %f' % (epoch, train_1.mean().item()))
##结果
![]
